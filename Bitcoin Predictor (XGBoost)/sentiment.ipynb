{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwclient\n",
    "import time\n",
    "import pickle \n",
    "from transformers import pipeline\n",
    "from statistics import mean\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve\n",
    "Improve datascraping for sentiment analysis:\n",
    "- look at news articles, tweets, google trends \n",
    "\n",
    "Find other correlated coin (e.g. etheruem or other crypto)\n",
    "\n",
    "Add in economic indicators (e.g inflation, bond yield, debt level, FX, USD, AUD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "# site = mwclient.Site(\"en.wikipedia.org\")\n",
    "# page = site.pages[\"Bitcoin\"]\n",
    "# revs = list(page.revisions())\n",
    "\n",
    "# Save the revs data so we don't have to scrape the web\n",
    "# Serialize and save to file\n",
    "#with open('revs.pk1', 'wb') as file:\n",
    "#    pickle.dump(revs, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('revid', 1193028719),\n",
       "             ('parentid', 1192928959),\n",
       "             ('user', 'Ravenpuff'),\n",
       "             ('timestamp',\n",
       "              time.struct_time(tm_year=2024, tm_mon=1, tm_mday=1, tm_hour=17, tm_min=51, tm_sec=34, tm_wday=0, tm_yday=1, tm_isdst=-1)),\n",
       "             ('comment', '/* top */ use {{as of}} in infobox')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load revs data from file\n",
    "with open('revs.pk1','rb') as file:\n",
    "    revs = pickle.load(file)\n",
    "\n",
    "revs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('revid', 275832581),\n",
       "             ('parentid', 0),\n",
       "             ('user', 'Pratyeka'),\n",
       "             ('timestamp',\n",
       "              time.struct_time(tm_year=2009, tm_mon=3, tm_mday=8, tm_hour=16, tm_min=41, tm_sec=7, tm_wday=6, tm_yday=67, tm_isdst=-1)),\n",
       "             ('comment', 'creation (stub)')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs = sorted(revs, key=lambda rev: rev[\"timestamp\"])\n",
    "revs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def find_sentiment(text): \n",
    "    '''Calculates the sentiment estimation of the text'''\n",
    "\n",
    "    # Run sentiment pipeline neural newtork on the input text (up to 250 characters)\n",
    "    sent = sentiment_classifier([text[:250]])[0]\n",
    "    score = sent[\"score\"]\n",
    "\n",
    "    # ... if it is a negative sentiment, make score negative\n",
    "    if sent[\"label\"] == \"NEGATIVE\":\n",
    "        score *= -1 \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% Complete\n",
      "0.56% Complete\n",
      "1.12% Complete\n",
      "1.69% Complete\n",
      "2.25% Complete\n",
      "2.81% Complete\n",
      "3.37% Complete\n",
      "3.94% Complete\n",
      "4.5% Complete\n",
      "5.06% Complete\n",
      "5.62% Complete\n",
      "6.19% Complete\n",
      "6.75% Complete\n",
      "7.31% Complete\n",
      "7.87% Complete\n",
      "8.44% Complete\n",
      "9.0% Complete\n",
      "9.56% Complete\n",
      "10.12% Complete\n",
      "10.69% Complete\n",
      "11.25% Complete\n",
      "11.81% Complete\n",
      "12.37% Complete\n",
      "12.94% Complete\n",
      "13.5% Complete\n",
      "14.06% Complete\n",
      "14.62% Complete\n",
      "15.19% Complete\n",
      "15.75% Complete\n",
      "16.31% Complete\n",
      "16.87% Complete\n",
      "17.44% Complete\n",
      "18.0% Complete\n",
      "18.56% Complete\n",
      "19.12% Complete\n",
      "19.69% Complete\n",
      "20.25% Complete\n",
      "20.81% Complete\n",
      "21.37% Complete\n",
      "21.94% Complete\n",
      "22.5% Complete\n",
      "23.06% Complete\n",
      "23.62% Complete\n",
      "24.19% Complete\n",
      "24.75% Complete\n",
      "25.31% Complete\n",
      "25.87% Complete\n",
      "26.44% Complete\n",
      "27.0% Complete\n",
      "27.56% Complete\n",
      "28.12% Complete\n",
      "28.69% Complete\n",
      "29.25% Complete\n",
      "29.81% Complete\n",
      "30.37% Complete\n",
      "30.94% Complete\n",
      "31.5% Complete\n",
      "32.06% Complete\n",
      "32.62% Complete\n",
      "33.19% Complete\n",
      "33.75% Complete\n",
      "34.31% Complete\n",
      "34.87% Complete\n",
      "35.44% Complete\n",
      "36.0% Complete\n",
      "36.56% Complete\n",
      "37.12% Complete\n",
      "37.68% Complete\n",
      "38.25% Complete\n",
      "38.81% Complete\n",
      "39.37% Complete\n",
      "39.93% Complete\n",
      "40.5% Complete\n",
      "41.06% Complete\n",
      "41.62% Complete\n",
      "42.18% Complete\n",
      "42.75% Complete\n",
      "43.31% Complete\n",
      "43.87% Complete\n",
      "44.43% Complete\n",
      "45.0% Complete\n",
      "45.56% Complete\n",
      "46.12% Complete\n",
      "46.68% Complete\n",
      "47.25% Complete\n",
      "47.81% Complete\n",
      "48.37% Complete\n",
      "48.93% Complete\n",
      "49.5% Complete\n",
      "50.06% Complete\n",
      "50.62% Complete\n",
      "51.18% Complete\n",
      "51.75% Complete\n",
      "52.31% Complete\n",
      "52.87% Complete\n",
      "53.43% Complete\n",
      "54.0% Complete\n",
      "54.56% Complete\n",
      "55.12% Complete\n",
      "55.68% Complete\n",
      "56.25% Complete\n",
      "56.81% Complete\n",
      "57.37% Complete\n",
      "57.93% Complete\n",
      "58.5% Complete\n",
      "59.06% Complete\n",
      "59.62% Complete\n",
      "60.18% Complete\n",
      "60.75% Complete\n",
      "61.31% Complete\n",
      "61.87% Complete\n",
      "62.43% Complete\n",
      "63.0% Complete\n",
      "63.56% Complete\n",
      "64.12% Complete\n",
      "64.68% Complete\n",
      "65.25% Complete\n",
      "65.81% Complete\n",
      "66.37% Complete\n",
      "66.93% Complete\n",
      "67.5% Complete\n",
      "68.06% Complete\n",
      "68.62% Complete\n",
      "69.18% Complete\n",
      "69.75% Complete\n",
      "70.31% Complete\n",
      "70.87% Complete\n",
      "71.43% Complete\n",
      "72.0% Complete\n",
      "72.56% Complete\n",
      "73.12% Complete\n",
      "73.68% Complete\n",
      "74.24% Complete\n",
      "74.81% Complete\n",
      "75.37% Complete\n",
      "75.93% Complete\n",
      "76.49% Complete\n",
      "77.06% Complete\n",
      "77.62% Complete\n",
      "78.18% Complete\n",
      "78.74% Complete\n",
      "79.31% Complete\n",
      "79.87% Complete\n",
      "80.43% Complete\n",
      "80.99% Complete\n",
      "81.56% Complete\n",
      "82.12% Complete\n",
      "82.68% Complete\n",
      "83.24% Complete\n",
      "83.81% Complete\n",
      "84.37% Complete\n",
      "84.93% Complete\n",
      "85.49% Complete\n",
      "86.06% Complete\n",
      "86.62% Complete\n",
      "87.18% Complete\n",
      "87.74% Complete\n",
      "88.31% Complete\n",
      "88.87% Complete\n",
      "89.43% Complete\n",
      "89.99% Complete\n",
      "90.56% Complete\n",
      "91.12% Complete\n",
      "91.68% Complete\n",
      "92.24% Complete\n",
      "92.81% Complete\n",
      "93.37% Complete\n",
      "93.93% Complete\n",
      "94.49% Complete\n",
      "95.06% Complete\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'comment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m edits[date][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate the sentiment of the edit and append to edits['sentiments'] list\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m comment \u001b[38;5;241m=\u001b[39m \u001b[43mrev\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m edits[date][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(find_sentiment(comment))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'comment'"
     ]
    }
   ],
   "source": [
    "edits = {}\n",
    "\n",
    "# Loop through every update and extract information\n",
    "for i, rev in enumerate(revs):\n",
    "    date = time.strftime(\"%Y-%m-%d\", rev[\"timestamp\"])\n",
    "\n",
    "    # If this is the first time seeing the date, add a new dictionary\n",
    "    if date not in edits:\n",
    "        edits[date] = dict(sentiments=list(), edit_count=0)\n",
    "\n",
    "    # Sum the number of edits per day\n",
    "    edits[date][\"edit_count\"] += 1\n",
    "\n",
    "    # Calculate the sentiment of the edit and append to edits['sentiments'] list\n",
    "    try:\n",
    "        comment = rev[\"comment\"]\n",
    "        edits[date][\"sentiments\"].append(find_sentiment(comment))\n",
    "    except KeyError:\n",
    "        edits[date][\"sentiments\"] = []\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        percent_complete = round(i/len(revs)*100,2)\n",
    "        print(f'{percent_complete}% Complete', end='\\r' flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy up the edits dictionary    \n",
    "for key in edits: \n",
    "\n",
    "    # Change the list of sentiment into an average rating and % negative votes, otherwise return 0\n",
    "    if len(edits[key][\"sentiments\"]) > 0: \n",
    "        edits[key][\"sentiment\"] = mean(edits[key][\"sentiments\"])\n",
    "        edits[key][\"neg_sentiment\"] = len([s for s in edits[key][\"sentiments\"] if s < 0])/len(edits[key][\"sentiments\"])\n",
    "    else:\n",
    "        edits[key][\"sentiment\"] = 0\n",
    "        edits[key][\"neg_sentiment\"] = 0\n",
    "\n",
    "    del edits[key][\"sentiments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the above so that we don't have to run it again (takes ~10 min)\n",
    "with open('edits_dict.pk1', 'wb') as file:\n",
    "    pickle.dump(edits, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results into dataframe\n",
    "edits_df = pd.Datarfame.from_dict(edits, orient=\"index\")\n",
    "edits_df.index = pd.to_datetime(edits_df.index)\n",
    "\n",
    "# We need a dataframe that has no missing days\n",
    "from datetime import datetime\n",
    "dates = pd.date_range(start=\"2009-03-08\", end=datetime.today())\n",
    "\n",
    "# Merge complete date df with edits df\n",
    "edits_df = edits_df.reindex(dates, fill_value=0)\n",
    "\n",
    "# Create a rolling average for the past month\n",
    "rolling_edits = edits_df.rolling(30).mean()\n",
    "rolling_edits = rolling_edits.dropna()\n",
    "rolling_edits.to_csv(\"wikipedia_edits.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
